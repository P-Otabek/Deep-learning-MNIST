{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd90e00",
   "metadata": {},
   "source": [
    "### MNIST Image classification using MLP\n",
    "The original code was taken from kaggle with some changes and improved with ChatGPT to use Adam Optimizer (originally optimization function was Gradient Descent) and to make it more dynamic (additional layers can be added to increase accuracy, beware of overfitting though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96fdddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d053f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.head()\n",
    "data = np.array(data)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data) # shuffle before splitting into dev and training sets\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b16438b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_dims):\n",
    "        self.parameters = {}\n",
    "        self.L = len(layer_dims)  # number of layers in the network\n",
    "        self.m = None  # will be set during training\n",
    "        self.layer_dims = layer_dims\n",
    "        self.v = {}\n",
    "        self.s = {}\n",
    "        \n",
    "        # Initialize parameters and Adam variables\n",
    "        for l in range(1, self.L):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "            \n",
    "            self.v[\"dW\" + str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\n",
    "            self.v[\"db\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "            \n",
    "            self.s[\"dW\" + str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\n",
    "            self.s[\"db\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def ReLU(Z):\n",
    "        return np.maximum(Z, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(Z):\n",
    "        expZ = np.exp(Z - np.max(Z))  # for numerical stability\n",
    "        return expZ / expZ.sum(axis=0, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ReLU_deriv(Z):\n",
    "        return Z > 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def one_hot(Y):\n",
    "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
    "        one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "        one_hot_Y = one_hot_Y.T\n",
    "        return one_hot_Y\n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        caches = {}\n",
    "        A = X\n",
    "        caches[\"A0\"] = X\n",
    "        \n",
    "        # ReLU activations for layers 1 to L-1\n",
    "        for l in range(1, self.L-1):\n",
    "            Z = self.parameters[\"W\" + str(l)].dot(A) + self.parameters[\"b\" + str(l)]\n",
    "            A = self.ReLU(Z)\n",
    "            caches[\"A\" + str(l)] = A\n",
    "            caches[\"Z\" + str(l)] = Z\n",
    "        \n",
    "        # Softmax activation for the last layer\n",
    "        ZL = self.parameters[\"W\" + str(self.L-1)].dot(A) + self.parameters[\"b\" + str(self.L-1)]\n",
    "        AL = self.softmax(ZL)\n",
    "        caches[\"A\" + str(self.L-1)] = AL\n",
    "        caches[\"Z\" + str(self.L-1)] = ZL\n",
    "        \n",
    "        return AL, caches\n",
    "    \n",
    "    def backward_prop(self, AL, Y, caches):\n",
    "        grads = {}\n",
    "        one_hot_Y = self.one_hot(Y)\n",
    "        \n",
    "        # Starting the backpropagation\n",
    "        dZ = AL - one_hot_Y\n",
    "        grads[\"dW\" + str(self.L-1)] = 1/self.m * dZ.dot(caches[\"A\" + str(self.L-2)].T)\n",
    "        grads[\"db\" + str(self.L-1)] = 1/self.m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        \n",
    "        for l in reversed(range(1, self.L-1)):\n",
    "            dZ_next = dZ\n",
    "            dZ = self.parameters[\"W\" + str(l+1)].T.dot(dZ_next) * self.ReLU_deriv(caches[\"Z\" + str(l)])\n",
    "            grads[\"dW\" + str(l)] = 1/self.m * dZ.dot(caches[\"A\" + str(l-1)].T)\n",
    "            grads[\"db\" + str(l)] = 1/self.m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def update_params_with_adam(self, grads, alpha, t, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        for l in range(1, self.L):\n",
    "            # Moving average of the gradients\n",
    "            self.v[\"dW\" + str(l)] = beta1 * self.v[\"dW\" + str(l)] + (1 - beta1) * grads[\"dW\" + str(l)]\n",
    "            self.v[\"db\" + str(l)] = beta1 * self.v[\"db\" + str(l)] + (1 - beta1) * grads[\"db\" + str(l)]\n",
    "            \n",
    "            # Compute bias-corrected first moment estimate\n",
    "            v_corrected_dW = self.v[\"dW\" + str(l)] / (1 - beta1**t)\n",
    "            v_corrected_db = self.v[\"db\" + str(l)] / (1 - beta1**t)\n",
    "            \n",
    "            # Moving average of the squared gradients\n",
    "            self.s[\"dW\" + str(l)] = beta2 * self.s[\"dW\" + str(l)] + (1 - beta2) * (grads[\"dW\" + str(l)]**2)\n",
    "            self.s[\"db\" + str(l)] = beta2 * self.s[\"db\" + str(l)] + (1 - beta2) * (grads[\"db\" + str(l)]**2)\n",
    "            \n",
    "            # Compute bias-corrected second raw moment estimate\n",
    "            s_corrected_dW = self.s[\"dW\" + str(l)] / (1 - beta2**t)\n",
    "            s_corrected_db = self.s[\"db\" + str(l)] / (1 - beta2**t)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.parameters[\"W\" + str(l)] -= alpha * v_corrected_dW / (np.sqrt(s_corrected_dW) + epsilon)\n",
    "            self.parameters[\"b\" + str(l)] -= alpha * v_corrected_db / (np.sqrt(s_corrected_db) + epsilon)\n",
    "    \n",
    "    def train(self, X, Y, alpha, iterations):\n",
    "        self.m = X.shape[1]\n",
    "        \n",
    "        for i in range(1, iterations+1):\n",
    "            AL, caches = self.forward_prop(X)\n",
    "            grads = self.backward_prop(AL, Y, caches)\n",
    "            self.update_params_with_adam(grads, alpha, i)\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                predictions = self.get_predictions(AL)\n",
    "                accuracy = self.get_accuracy(predictions, Y)\n",
    "                print(f\"Iteration {i} - Accuracy: {accuracy}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_predictions(A):\n",
    "        return np.argmax(A, axis=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_accuracy(predictions, Y):\n",
    "        return np.sum(predictions == Y) / Y.size\n",
    "    \n",
    "    def make_predictions(self, X):\n",
    "        AL, _ = self.forward_prop(X)\n",
    "        predictions = self.get_predictions(AL)\n",
    "        return predictions\n",
    "\n",
    "    def compute_accuracy(self, X, Y):\n",
    "        predictions = self.make_predictions(X)\n",
    "        return self.get_accuracy(predictions, Y)\n",
    "    def compute_loss(self, AL, Y):\n",
    "        \"\"\"\n",
    "        Compute the cross-entropy loss.\n",
    "        \n",
    "        Parameters:\n",
    "        - AL: Activations from the last layer, representing probability vector corresponding to label predictions.\n",
    "        - Y: True labels vector.\n",
    "        \n",
    "        Returns:\n",
    "        - Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        one_hot_Y = self.one_hot(Y)\n",
    "        m = Y.size\n",
    "        loss = -1/m * np.sum(one_hot_Y * np.log(AL + 1e-8))  # adding a small value to avoid log(0)\n",
    "        return loss\n",
    "    def compute_loss_on_dataset(self, X, Y):\n",
    "        \"\"\"\n",
    "        Compute the loss on a given dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data.\n",
    "        - Y: True labels vector.\n",
    "        \n",
    "        Returns:\n",
    "        - Cross-entropy loss.\n",
    "        \"\"\"\n",
    "        AL, _ = self.forward_prop(X)\n",
    "        return self.compute_loss(AL, Y)\n",
    "    \n",
    "def test_prediction(index, model):\n",
    "    current_image = X_train[:, index, None]\n",
    "    prediction = model.make_predictions(current_image)\n",
    "    label = Y_train[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "    \n",
    "    current_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(current_image, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdcb487d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20 - Accuracy: 0.5396341463414634\n",
      "Iteration 40 - Accuracy: 0.782219512195122\n",
      "Iteration 60 - Accuracy: 0.8672439024390244\n",
      "Iteration 80 - Accuracy: 0.8977560975609756\n",
      "Iteration 100 - Accuracy: 0.9124634146341464\n",
      "Iteration 120 - Accuracy: 0.9236585365853659\n",
      "Iteration 140 - Accuracy: 0.9318780487804879\n",
      "Iteration 160 - Accuracy: 0.9394634146341463\n",
      "Iteration 180 - Accuracy: 0.9463170731707317\n",
      "Iteration 200 - Accuracy: 0.9527560975609756\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([784, 128, 128, 10])\n",
    "nn.train(X_train, Y_train, alpha=0.001, iterations=201) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a986b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  [0]\n",
      "Label:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGdCAYAAAC7EMwUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa90lEQVR4nO3df2xV9f3H8dcV6+XH2ps00N5bga4ayDZgZCIDG39UJg2dogiboNlWYkJQCglDxmSMUDShhgxmAsNlxiBkMnARGJlE7QYtboylMgyMOQJapASaDoL3VsBLkM/3D8L9em2pfC738u69fT6Sk3DPPS/Ou8dDX57ee08DzjknAAAM3GQ9AACg56KEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYOZm6wG+7NKlSzpx4oTy8/MVCASsxwEAeHLOqb29XSUlJbrppq6vdbpdCZ04cUKDBg2yHgMAcJ1aWlo0cODALrfpdj+Oy8/Ptx4BAJAG1/L9PGMltGbNGpWVlal3794aNWqU3n333WvK8SM4AMgN1/L9PCMltGnTJs2dO1eLFi3Svn37dM8996iqqkrHjh3LxO4AAFkqkIm7aI8ZM0Z33HGHXnrppcS6b37zm5o0aZLq6uq6zMZiMYVCoXSPBAC4waLRqAoKCrrcJu1XQhcuXNDevXtVWVmZtL6yslK7d+/usH08HlcsFktaAAA9Q9pL6NSpU/r8889VXFyctL64uFitra0dtq+rq1MoFEosvDMOAHqOjL0x4csvSDnnOn2RauHChYpGo4mlpaUlUyMBALqZtH9OqH///urVq1eHq562trYOV0eSFAwGFQwG0z0GACALpP1K6JZbbtGoUaNUX1+ftL6+vl7l5eXp3h0AIItl5I4J8+bN049//GPdeeeduuuuu/S73/1Ox44d01NPPZWJ3QEAslRGSmjq1Kk6ffq0nnvuOZ08eVLDhw/X9u3bVVpamondAQCyVEY+J3Q9+JwQAOQGk88JAQBwrSghAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYOZm6wGA7qSoqMg78/3vf98789Of/tQ7s23bNu/Mxo0bvTOSdOjQIe/MxYsXU9oXejauhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJgJOOec9RBfFIvFFAqFrMdAlquoqEgp98orr3hnSktLU9qXr0Ag4J1J9Z/3jh07vDNPPvmkd+b48ePeGWSPaDSqgoKCLrfhSggAYIYSAgCYSXsJ1dbWKhAIJC3hcDjduwEA5ICM/FK7YcOG6S9/+Uvica9evTKxGwBAlstICd18881c/QAAvlJGXhM6fPiwSkpKVFZWpmnTpumjjz666rbxeFyxWCxpAQD0DGkvoTFjxmj9+vV6++239fLLL6u1tVXl5eU6ffp0p9vX1dUpFAollkGDBqV7JABAN5X2EqqqqtKUKVM0YsQIPfDAA3rzzTclSevWret0+4ULFyoajSaWlpaWdI8EAOimMvKa0Bf169dPI0aM0OHDhzt9PhgMKhgMZnoMAEA3lPHPCcXjcX3wwQeKRCKZ3hUAIMukvYTmz5+vxsZGNTc365///Kd+8IMfKBaLqbq6Ot27AgBkubT/OO748eN6/PHHderUKQ0YMEBjx47Vnj17btj9tQAA2YMbmKLbe/DBB70zP/nJT1La15QpU1LK+VqzZo13Zv/+/d6Z5cuXe2ckKS8vzztz8eJF78zevXu9M2fOnPHOvP76694ZSfrjH/+YUg6XcQNTAEC3RgkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwEzGf6kd8EW33nqrd+bXv/61d+a2227zzkjSxx9/7J35+9//7p355S9/6Z2JxWLembfeess7I0mDBw/2zqxatco7U1FR4Z05f/68d+bAgQPeGdwYXAkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMwEnHPOeogvisViCoVC1mPgGqRyR+xU7ur8rW99yztz5MgR74wkPfDAA96ZlpaWlPaVa/r16+edKS0t9c68+uqr3pk+ffp4ZyRp165d3pmampqU9pWLotGoCgoKutyGKyEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmuIEpUrqJpCTV19d7Z26//XbvzLJly7wzixcv9s4gdy1dujSl3Pz5870zDz/8sHfmr3/9q3cmG3ADUwBAt0YJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMNzCF1q9fn1LuiSee8M6sWLHCO/Pzn//cOwN80YABA1LK/fe///XOvP/++96Z733ve96ZbMANTAEA3RolBAAw411Cu3bt0sSJE1VSUqJAIKCtW7cmPe+cU21trUpKStSnTx9VVFTo4MGD6ZoXAJBDvEvo7NmzGjlypFavXt3p88uXL9fKlSu1evVqNTU1KRwOa/z48Wpvb7/uYQEAueVm30BVVZWqqqo6fc45pxdffFGLFi3S5MmTJUnr1q1TcXGxNmzYoJkzZ17ftACAnJLW14Sam5vV2tqqysrKxLpgMKj77rtPu3fv7jQTj8cVi8WSFgBAz5DWEmptbZUkFRcXJ60vLi5OPPdldXV1CoVCiWXQoEHpHAkA0I1l5N1xgUAg6bFzrsO6KxYuXKhoNJpYWlpaMjESAKAb8n5NqCvhcFjS5SuiSCSSWN/W1tbh6uiKYDCoYDCYzjEAAFkirVdCZWVlCofDqq+vT6y7cOGCGhsbVV5ens5dAQBygPeV0KeffqojR44kHjc3N+v9999XYWGhBg8erLlz52rZsmUaMmSIhgwZomXLlqlv374p3eIFAJDbvEvovffe0/333594PG/ePElSdXW1Xn31VS1YsEDnz5/XrFmzdObMGY0ZM0bvvPOO8vPz0zc1ACAncAPTHPP88897ZxYsWJDSvv797397ZyZMmOCd+d///uedAdJhypQp3plZs2Z5Z7iBKQAABighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZtL6m1WRXv369fPOPPjgg96Z8+fPe2ck6bHHHvPOcEdsZJM///nP3pkf/ehH3pmBAwd6Z44fP+6d6Y64EgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGG5h2Y1//+te9M9/+9re9M6tWrfLOSNKHH36YUg7IFvF43Dtz4cIF70x5ebl35vXXX/fOdEdcCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADDDDUxzTCAQ8M4sW7YsA5MA2S8/P987853vfMc7s3TpUu9MruBKCABghhICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBluYJpjnHPWIwA5Y9iwYd6ZVG56GovFvDO5gishAIAZSggAYMa7hHbt2qWJEyeqpKREgUBAW7duTXp++vTpCgQCScvYsWPTNS8AIId4l9DZs2c1cuRIrV69+qrbTJgwQSdPnkws27dvv64hAQC5yfuNCVVVVaqqqupym2AwqHA4nPJQAICeISOvCTU0NKioqEhDhw7VjBkz1NbWdtVt4/G4YrFY0gIA6BnSXkJVVVV67bXXtGPHDq1YsUJNTU0aN26c4vF4p9vX1dUpFAollkGDBqV7JABAN5X2zwlNnTo18efhw4frzjvvVGlpqd58801Nnjy5w/YLFy7UvHnzEo9jsRhFBAA9RMY/rBqJRFRaWqrDhw93+nwwGFQwGMz0GACAbijjnxM6ffq0WlpaFIlEMr0rAECW8b4S+vTTT3XkyJHE4+bmZr3//vsqLCxUYWGhamtrNWXKFEUiER09elS/+MUv1L9/fz366KNpHRwAkP28S+i9997T/fffn3h85fWc6upqvfTSSzpw4IDWr1+vTz75RJFIRPfff782bdqU0v2UAAC5zbuEKioqurxJ5ttvv31dA+H/jR492noEICf07t07pdzixYu9Mz/72c+8M8ePH/fO5AruHQcAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMJPx36yK1I0aNcp6BKDb6devn3dm/vz5Ke1r3Lhx3pnly5entK+eiishAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZriBKfSrX/0qpdxzzz3nnfnwww9T2hdwxUMPPeSdWbx4cUr7mjlzpnemsbExpX31VFwJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMBNwzjnrIb4oFospFApZj9EtDBs2zDuzf//+DEzSuSeffNI7s3HjRu9MPB73ziA7TJo0yTuzZcsW78wnn3zinZGk2267zTtz5syZlPaVi6LRqAoKCrrchishAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZm62HgBXd/ToUe/MG2+84Z2ZPHmyd0aSnn/+ee9MSUmJd6aurs47g+vTu3dv78yqVau8M1OnTvXOpHIz0h/+8IfeGYmbkd4IXAkBAMxQQgAAM14lVFdXp9GjRys/P19FRUWaNGmSDh06lLSNc061tbUqKSlRnz59VFFRoYMHD6Z1aABAbvAqocbGRtXU1GjPnj2qr6/XxYsXVVlZqbNnzya2Wb58uVauXKnVq1erqalJ4XBY48ePV3t7e9qHBwBkN683Jrz11ltJj9euXauioiLt3btX9957r5xzevHFF7Vo0aLEi93r1q1TcXGxNmzYoJkzZ6ZvcgBA1ruu14Si0agkqbCwUJLU3Nys1tZWVVZWJrYJBoO67777tHv37k7/jng8rlgslrQAAHqGlEvIOad58+bp7rvv1vDhwyVJra2tkqTi4uKkbYuLixPPfVldXZ1CoVBiGTRoUKojAQCyTMolNHv2bO3fv19/+MMfOjwXCASSHjvnOqy7YuHChYpGo4mlpaUl1ZEAAFkmpQ+rzpkzR9u2bdOuXbs0cODAxPpwOCzp8hVRJBJJrG9ra+twdXRFMBhUMBhMZQwAQJbzuhJyzmn27NnavHmzduzYobKysqTny8rKFA6HVV9fn1h34cIFNTY2qry8PD0TAwByhteVUE1NjTZs2KA//elPys/PT7zOEwqF1KdPHwUCAc2dO1fLli3TkCFDNGTIEC1btkx9+/bVE088kZEvAACQvbxK6KWXXpIkVVRUJK1fu3atpk+fLklasGCBzp8/r1mzZunMmTMaM2aM3nnnHeXn56dlYABA7gg455z1EF8Ui8UUCoWsx8haqby+9uyzz6a0rwULFnhnLl265J05d+6cdybV0/rpp5/2zuTl5Xlnli5d6p252pt7utLZG4euRd++fb0zzzzzjHemqanJO/Ovf/3LO1NTU+OdwfWLRqMqKCjochvuHQcAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMMNdtJGyJUuWeGemTZvmnRk6dKh3JtXTOh6Pe2dOnTrlnbn11lu9M6ncRbub/fPu4Mqvh/ExZ86cDEyCTOAu2gCAbo0SAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgBlKCABghhICAJihhAAAZighAIAZbmCKG6qwsNA789hjj3lnHn74Ye+MJN1zzz3emXfeecc7c/vtt3tnRowY4Z1J9Z/32rVrvTPbtm3zzjQ1NXlnWltbvTOwwQ1MAQDdGiUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADPcwBQAkBHcwBQA0K1RQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMCMVwnV1dVp9OjRys/PV1FRkSZNmqRDhw4lbTN9+nQFAoGkZezYsWkdGgCQG7xKqLGxUTU1NdqzZ4/q6+t18eJFVVZW6uzZs0nbTZgwQSdPnkws27dvT+vQAIDccLPPxm+99VbS47Vr16qoqEh79+7Vvffem1gfDAYVDofTMyEAIGdd12tC0WhUklRYWJi0vqGhQUVFRRo6dKhmzJihtra2q/4d8XhcsVgsaQEA9AwB55xLJeic0yOPPKIzZ87o3XffTazftGmTvva1r6m0tFTNzc1avHixLl68qL179yoYDHb4e2pra7V06dLUvwIAQLcUjUZVUFDQ9UYuRbNmzXKlpaWupaWly+1OnDjh8vLy3BtvvNHp85999pmLRqOJpaWlxUliYWFhYcnyJRqNfmWXeL0mdMWcOXO0bds27dq1SwMHDuxy20gkotLSUh0+fLjT54PBYKdXSACA3OdVQs45zZkzR1u2bFFDQ4PKysq+MnP69Gm1tLQoEomkPCQAIDd5vTGhpqZGv//977Vhwwbl5+ertbVVra2tOn/+vCTp008/1fz58/WPf/xDR48eVUNDgyZOnKj+/fvr0UcfzcgXAADIYj6vA+kqP/dbu3atc865c+fOucrKSjdgwACXl5fnBg8e7Kqrq92xY8eueR/RaNT855gsLCwsLNe/XMtrQim/Oy5TYrGYQqGQ9RgAgOt0Le+O495xAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAzlBAAwAwlBAAwQwkBAMxQQgAAM5QQAMAMJQQAMEMJAQDMUEIAADOUEADADCUEADBDCQEAzFBCAAAz3a6EnHPWIwAA0uBavp93uxJqb2+3HgEAkAbX8v084LrZpcelS5d04sQJ5efnKxAIJD0Xi8U0aNAgtbS0qKCgwGhCexyHyzgOl3EcLuM4XNYdjoNzTu3t7SopKdFNN3V9rXPzDZrpmt10000aOHBgl9sUFBT06JPsCo7DZRyHyzgOl3EcLrM+DqFQ6Jq263Y/jgMA9ByUEADATFaVUDAY1JIlSxQMBq1HMcVxuIzjcBnH4TKOw2XZdhy63RsTAAA9R1ZdCQEAcgslBAAwQwkBAMxQQgAAM1lVQmvWrFFZWZl69+6tUaNG6d1337Ue6Yaqra1VIBBIWsLhsPVYGbdr1y5NnDhRJSUlCgQC2rp1a9LzzjnV1taqpKREffr0UUVFhQ4ePGgzbAZ91XGYPn16h/Nj7NixNsNmSF1dnUaPHq38/HwVFRVp0qRJOnToUNI2PeF8uJbjkC3nQ9aU0KZNmzR37lwtWrRI+/bt0z333KOqqiodO3bMerQbatiwYTp58mRiOXDggPVIGXf27FmNHDlSq1ev7vT55cuXa+XKlVq9erWampoUDoc1fvz4nLsP4VcdB0maMGFC0vmxffv2Gzhh5jU2NqqmpkZ79uxRfX29Ll68qMrKSp09ezaxTU84H67lOEhZcj64LPHd737XPfXUU0nrvvGNb7hnn33WaKIbb8mSJW7kyJHWY5iS5LZs2ZJ4fOnSJRcOh90LL7yQWPfZZ5+5UCjkfvvb3xpMeGN8+Tg451x1dbV75JFHTOax0tbW5iS5xsZG51zPPR++fBycy57zISuuhC5cuKC9e/eqsrIyaX1lZaV2795tNJWNw4cPq6SkRGVlZZo2bZo++ugj65FMNTc3q7W1NencCAaDuu+++3rcuSFJDQ0NKioq0tChQzVjxgy1tbVZj5RR0WhUklRYWCip554PXz4OV2TD+ZAVJXTq1Cl9/vnnKi4uTlpfXFys1tZWo6luvDFjxmj9+vV6++239fLLL6u1tVXl5eU6ffq09Whmrvz37+nnhiRVVVXptdde044dO7RixQo1NTVp3Lhxisfj1qNlhHNO8+bN0913363hw4dL6pnnQ2fHQcqe86Hb3UW7K1/+1Q7OuQ7rcllVVVXizyNGjNBdd92l22+/XevWrdO8efMMJ7PX088NSZo6dWriz8OHD9edd96p0tJSvfnmm5o8ebLhZJkxe/Zs7d+/X3/72986PNeTzoerHYdsOR+y4kqof//+6tWrV4f/k2lra+vwfzw9Sb9+/TRixAgdPnzYehQzV94dyLnRUSQSUWlpaU6eH3PmzNG2bdu0c+fOpF/90tPOh6sdh8501/MhK0rolltu0ahRo1RfX5+0vr6+XuXl5UZT2YvH4/rggw8UiUSsRzFTVlamcDicdG5cuHBBjY2NPfrckKTTp0+rpaUlp84P55xmz56tzZs3a8eOHSorK0t6vqecD191HDrTbc8HwzdFeNm4caPLy8tzr7zyivvPf/7j5s6d6/r16+eOHj1qPdoN88wzz7iGhgb30UcfuT179riHHnrI5efn5/wxaG9vd/v27XP79u1zktzKlSvdvn373Mcff+ycc+6FF15woVDIbd682R04cMA9/vjjLhKJuFgsZjx5enV1HNrb290zzzzjdu/e7Zqbm93OnTvdXXfd5W699dacOg5PP/20C4VCrqGhwZ08eTKxnDt3LrFNTzgfvuo4ZNP5kDUl5Jxzv/nNb1xpaam75ZZb3B133JH0dsSeYOrUqS4Sibi8vDxXUlLiJk+e7A4ePGg9Vsbt3LnTSeqwVFdXO+cuvy13yZIlLhwOu2Aw6O6991534MAB26EzoKvjcO7cOVdZWekGDBjg8vLy3ODBg111dbU7duyY9dhp1dnXL8mtXbs2sU1POB++6jhk0/nAr3IAAJjJiteEAAC5iRICAJihhAAAZighAIAZSggAYIYSAgCYoYQAAGYoIQCAGUoIAGCGEgIAmKGEAABmKCEAgJn/A+t8WgM/QK6QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction(6, nn)  # Test prediction for the 6th example in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec8469b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 93.30%\n",
      "Loss on test dataset: 19.00%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = nn.compute_accuracy(X_dev, Y_dev)\n",
    "print(f\"Accuracy on test dataset: {test_accuracy * 100:.2f}%\")\n",
    "test_loss = nn.compute_loss_on_dataset(X_dev, Y_dev)\n",
    "print(f\"Loss on test dataset: {test_loss *100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe01b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
